{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of Inital Decision Tree Model\n",
    "\n",
    "Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries and set display options \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "pd.set_option(\"display.max_rows\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe including all covariables to explore for initial model \n",
    "df = pd.read_csv('CompleteBaselineAcuteDataVoxtox.csv')\n",
    "recurrence_df = pd.read_csv('RecurrencePatientIDs.csv')\n",
    "df = pd.merge(df, recurrence_df, how='outer', on=['PATIENT_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LOCAL_RECURRENCE'].fillna(0, inplace=True)\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding for Cateogorical Variables \n",
    "- This replaces categorical string values with numerical ones which is easier for ML algorithms to interpret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for one hot encoding \n",
    "def binary_sex(sex):\n",
    "    if sex == \"M\":\n",
    "        new_sex = 0 \n",
    "    else: \n",
    "        new_sex = 1\n",
    "    \n",
    "    return new_sex\n",
    "        \n",
    "    \n",
    "def binary_definitive(definitive_rt):\n",
    "    if definitive_rt == 'Definitive':\n",
    "        new_defin = 1\n",
    "    else:\n",
    "        new_defin = 0\n",
    "        \n",
    "    return new_defin\n",
    "\n",
    "\n",
    "def site_categories(primary_site):\n",
    "    if primary_site == 'oropharynx':\n",
    "        site = '1'\n",
    "    elif primary_site == 'oral cavity':\n",
    "        site = '2'\n",
    "    elif primary_site == 'hypopharynx':\n",
    "        site = '3'\n",
    "    elif primary_site == 'larynx':\n",
    "        site = '4'\n",
    "    elif primary_site == 'nasopharynx':\n",
    "        site = '5'\n",
    "    else: \n",
    "        site = \"6\"\n",
    "    return site \n",
    "\n",
    "df['BINARY_SEX'] = df.apply(lambda x: binary_sex(x['SEX']),axis=1) \n",
    "df['BINARY_DEFINITIVE'] = df.apply(lambda x: binary_definitive(x['DEFINITIVE_RT']),axis=1) \n",
    "df['CATEGORICAL_SITE'] = df.apply(lambda x: site_categories(x['PRIMARY_SITE']),axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Correlations Between Variables\n",
    "Use a correlation matrix that uses both size and gradient to determine the correlations between the variables selected from the voxtox data\n",
    "- Max Dose is highly correlated with BED and Definitive RT - so I will just keep maxdose \n",
    "- High correlation between the peak values and STATScore - just keep STATscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heatmap import heatmap, corrplot\n",
    "    \n",
    "check_corr = df[['HEIGHT', 'WEIGHT', 'SMOKER',\n",
    "        'ALCOHOL', 'PRIMARY_SURGERY',\n",
    "       'NEOADJUVANT_CHEMO', 'AGE', 'BINARY_SEX', 'FRACTIONS', 'MAX_DOSE', 'BED',\n",
    "       'DYS_AUC', 'XERO_AUC', 'MUCO_AUC', 'DYS_PEAK', 'XERO_PEAK', 'MUCO_PEAK',\n",
    "       'STAT_SCORE', 'CATEGORICAL_SITE', 'TNM_STAGE', 'BINARY_DEFINITIVE',\n",
    "       'DRY_MOUTH_BASELINE', 'DYSPHAGIA_BASELINE', 'ORAL_MUCOSITIS_BASELINE', 'LOCAL_RECURRENCE']]\n",
    "plt.figure(figsize=(10, 10))\n",
    "corrplot(check_corr.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Decision Tree Model \n",
    "- Create df for the predictive variables and the class to predict as x,y\n",
    "- Impute the missing information \n",
    "    - iterative imputation models each feature as a function of other features e.g. regression problem \n",
    "    - Each feature is imputed sequentially, allowing prior imputed values to be used as part of the prediction\n",
    "    - \n",
    "- Use the standard scaler to normalise values between 0-1 for all categories\n",
    "    - This overcomes any differences between scales, ranges or measured units by standardising to (μ = 0, σ = 1)\n",
    "    - These differences can create problems for machine learning models that rely on distance calculations\n",
    "    - After scaling all variables should contribute equally to the model, avoiding biasing \n",
    "- Train, Test split for DT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[['HEIGHT', 'WEIGHT', 'SMOKER',\n",
    "        'ALCOHOL', 'PRIMARY_SURGERY',\n",
    "       'NEOADJUVANT_CHEMO', 'AGE', 'BINARY_SEX', \n",
    "        'FRACTIONS', 'MAX_DOSE', \n",
    "       'DYS_AUC', 'XERO_AUC', 'MUCO_AUC',\n",
    "       'STAT_SCORE', 'CATEGORICAL_SITE', 'TNM_STAGE',\n",
    "       'DRY_MOUTH_BASELINE', 'DYSPHAGIA_BASELINE', \n",
    "        'ORAL_MUCOSITIS_BASELINE']]\n",
    "\n",
    "y = df[['LOCAL_RECURRENCE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the amount of missing data in each column \n",
    "for i in list(x.columns):\n",
    "    # count number of rows with missing values\n",
    "    n_miss = x[[i]].isnull().sum()\n",
    "    perc = n_miss / x.shape[0] * 100\n",
    "    print('> %s, Missing: %d (%.1f%%)' % (i, n_miss, perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x['WEIGHT'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove known values from the df and run the imputation \n",
    "- compare the imputed values and the true values that have been removed to determine the success of the imputation \n",
    "- do this for values close the mean of the cohort, and outlier values for full test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = x.copy()\n",
    "new_df['WEIGHT'] = new_df['WEIGHT'].replace(82.8, np.nan) #0\n",
    "new_df['WEIGHT'] = new_df['WEIGHT'].replace(129.1, np.nan) #225\n",
    "new_df['HEIGHT'] = new_df['HEIGHT'].replace(153, np.nan) #3\n",
    "new_df['HEIGHT'] = new_df['HEIGHT'].replace(173.734065, np.nan) #228"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = IterativeImputer(max_iter=100)\n",
    "imputer.fit(new_df)\n",
    "x_transform_new = imputer.transform(new_df)\n",
    "print('Missing: %d' % sum(np.isnan(x_transform_new).flatten()))\n",
    "imputed_test_x = pd.DataFrame(x_transform_new, columns=list(x.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_test_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Imputation for the full df without removing any values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the data using the Iterative Imputer method\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10)\n",
    "imputer.fit(x)\n",
    "x_transform = imputer.transform(x)\n",
    "print('Missing: %d' % sum(np.isnan(x_transform).flatten()))\n",
    "imputed_x = pd.DataFrame(x_transform, columns=list(x.columns))\n",
    "#imputed_x.to_csv('ImputedVoxToxData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_x\n",
    "imputed_df = pd.concat([imputed_x, y], axis=1)\n",
    "y=list(y['LOCAL_RECURRENCE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Decision Tree Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to determine use the train, test split for the decision tree model and its metrics \n",
    "\n",
    "def decision_tree(imputed_x, y):\n",
    "    predictions = {}\n",
    "    error = []\n",
    "    accuracy = []\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    for i in range(1,6): \n",
    "        x_train, x_test, y_train, y_test = train_test_split(imputed_x, y, test_size = 0.3)\n",
    "        scaler.fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "        dt_model = DecisionTreeClassifier(criterion = \"gini\", max_depth=None, class_weight={0:0.05, 1:0.95})\n",
    "        dt_model.fit(x_train, y_train)\n",
    "        y_pred = dt_model.predict(x_test)\n",
    "        error.append(np.mean(y_pred != y_test))\n",
    "        accuracy.append(sklearn.metrics.accuracy_score(y_test, y_pred))\n",
    "        print(sklearn.metrics.accuracy_score(y_test, y_pred))\n",
    "        con_matrix = confusion_matrix(y_test, y_pred)\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        print(con_matrix, class_report)\n",
    "        predictions[i] = [y_test, y_pred]\n",
    "        \n",
    "    #return predictions, error\n",
    "    return accuracy, predictions\n",
    "\n",
    "\n",
    "def decision_tree_metrics(predictions):\n",
    "    accuracy = []\n",
    "    brier_score = []\n",
    "    precision = []\n",
    "    f1_score = []\n",
    "    roc_auc = []\n",
    "    \n",
    "    \n",
    "    for key in predictions.keys(): \n",
    "        con_matrix = confusion_matrix(predictions[key][0], predictions[key][1])\n",
    "        class_report = classification_report(predictions[key][0], predictions[key][1])\n",
    "        print(con_matrix, class_report)\n",
    "        print(sklearn.metrics.accuracy_score(predictions[key][0], predictions[key][1]))\n",
    "        print(sklearn.metrics.brier_score_loss(predictions[key][0], predictions[key][1]))\n",
    "        \n",
    "        accuracy.append(sklearn.metrics.accuracy_score(predictions[key][0], predictions[key][1]))\n",
    "        brier_score.append(sklearn.metrics.brier_score_loss(predictions[key][0], predictions[key][1]))\n",
    "        precision.append(sklearn.metrics.precision_score(predictions[key][0], predictions[key][1]))\n",
    "        f1_score.append(sklearn.metrics.f1_score(predictions[key][0], predictions[key][1]))\n",
    "        false_positive_rate, true_positive_rate, thresholds = roc_curve(predictions[key][0], predictions[key][1])\n",
    "        auc_value = auc(false_positive_rate, true_positive_rate)\n",
    "        roc_auc.append(auc_value)\n",
    "\n",
    "    \n",
    "    return accuracy, brier_score, precision, f1_score, roc_auc   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the decision tree functions, where the metrics for each model are printed \n",
    "accuracy, predictions = decision_tree(imputed_x, y)\n",
    "accuracy, brier_score, precision, f1_score, roc_auc = decision_tree_metrics(predictions)\n",
    "print(np.mean(accuracy), np.mean(brier_score), np.mean(precision), np.mean(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DT Hyperparameter Optimisation \n",
    "1. Max Depth of the Tree, how many branches we can include in the model \n",
    "2. Max features that can be included for the model at each node \n",
    "\n",
    "Visualise the max depth and max features against the AUC metric to find the optimal value for each parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = np.linspace(1, 32, 32, endpoint=True)\n",
    "train_results = []\n",
    "test_results = []\n",
    "for max_depth in max_depths:\n",
    "    dt = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    dt.fit(x_train, y_train)\n",
    "    train_pred = dt.predict(x_train)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    # Add auc score to previous train results\n",
    "    train_results.append(roc_auc)\n",
    "    y_pred = dt.predict(x_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    # Add auc score to previous test results\n",
    "    test_results.append(roc_auc)\n",
    "    \n",
    "    \n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "line1, = plt.plot(max_depths, train_results, 'b', label='Train AUC')\n",
    "line2, = plt.plot(max_depths, test_results, 'r', label='Test AUC')\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('AUC Score')\n",
    "plt.xlabel('Tree Depth')\n",
    "plt.show()\n",
    "plt.savefig('treedepthtest2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = list(range(1, x.shape[1]))\n",
    "train_results = []\n",
    "test_results = []\n",
    "for max_feature in max_features:\n",
    "    dt = DecisionTreeClassifier(max_features=max_feature)\n",
    "    dt.fit(x_train, y_train)\n",
    "    train_pred = dt.predict(x_train)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    train_results.append(roc_auc)\n",
    "    y_pred = dt.predict(x_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    test_results.append(roc_auc)\n",
    "    \n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "line1, = plt.plot(max_features, train_results, 'b', label='Train AUC')\n",
    "line2, = plt.plot(max_features, test_results, 'r', label='Test AUC')\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('AUC Score')\n",
    "plt.xlabel('Max Features')\n",
    "plt.show()\n",
    "plt.savefig('maxfeaturetest1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Optimised Hyperparameters and Standardised Scaling for final output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employ feature scaling to ensure variables lie within comparable ranges \n",
    "scaler = StandardScaler()\n",
    "x_train, x_test, y_train, y_test = train_test_split(imputed_x, y, test_size = 0.3)\n",
    "check_x_test1 = x_test\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# Run the DT model using the scaled data \n",
    "dt_model = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5, \\\n",
    "                                    max_features=10, splitter='best')\n",
    "\n",
    "dt_model.fit(x_train, y_train)\n",
    "y_pred = dt_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine area under the curve \n",
    "from sklearn.metrics import roc_curve, auc\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the final decision tree \n",
    "import sklearn.tree\n",
    "plt.figure(figsize=(10, 10))\n",
    "sklearn.tree.plot_tree(dt_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
